#!/usr/bin/env python3
"""
Quick demonstration of the optimized liquid-spiking neural network training.

This script shows a short training run to demonstrate the optimization
improvements without requiring a full training session.
"""

import sys
import os
import time
import torch
import numpy as np

# Add parent directory to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from main import (
    create_llm_config, LiquidSpikingNetwork, LiquidSpikingTrainer,
    DatasetFactory
)
from torch.utils.data import DataLoader

def quick_optimization_demo():
    """Run a quick demo showing the optimizations in action."""
    print("ğŸš€ QUICK OPTIMIZATION DEMONSTRATION")
    print("=" * 55)
    print("This demo shows the advanced optimizations working")
    print("with a small dataset for quick results.")
    print("=" * 55)
    
    # Get optimized configuration
    config = create_llm_config()
    
    print(f"\nğŸ“‹ Optimized Configuration:")
    print(f"   â€¢ Sequence length: {config.sequence_length} tokens")
    print(f"   â€¢ Batch size: {config.batch_size}")
    print(f"   â€¢ Learning rate: {config.learning_rate}")
    print(f"   â€¢ Weight decay: {config.weight_decay}")
    print(f"   â€¢ Mixed precision: {config.mixed_precision}")
    
    # Create small dataset for quick demo
    print(f"\nğŸ“š Creating small dataset for demo...")
    
    # Small dataset for quick demonstration
    train_dataset, tokenizer = DatasetFactory.create_llm_dataset(
        vocab_size=config.output_dim,
        seq_length=config.sequence_length,
        num_samples=500  # Small for quick demo
    )
    
    val_dataset, _ = DatasetFactory.create_llm_dataset(
        vocab_size=config.output_dim,
        seq_length=config.sequence_length,
        num_samples=100,  # Small validation set
        tokenizer_name='gpt2'
    )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        pin_memory=True,
        num_workers=2
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        pin_memory=True,
        num_workers=2
    )
    
    print(f"   Training samples: {len(train_dataset)}")
    print(f"   Validation samples: {len(val_dataset)}")
    
    # Create optimized model
    print(f"\nğŸ§  Creating optimized model...")
    model = LiquidSpikingNetwork(config)
    trainer = LiquidSpikingTrainer(model, config)
    
    total_params = sum(p.numel() for p in model.parameters())
    print(f"   Parameters: {total_params:,}")
    print(f"   Accumulation steps: {trainer.accumulation_steps}")
    print(f"   EMA decay: {trainer.ema_decay}")
    
    # Quick training demo (1 epoch)
    print(f"\nğŸ¯ Running 1 epoch demonstration...")
    start_time = time.time()
    
    # Train one epoch to show optimizations
    train_loss, grad_norm = trainer.train_epoch(train_loader)
    val_loss, val_accuracy, is_best = trainer.validate(val_loader)
    
    epoch_time = time.time() - start_time
    
    print(f"\nğŸ“Š Demonstration Results:")
    print(f"   Training loss: {train_loss:.4f}")
    print(f"   Validation loss: {val_loss:.4f}")
    print(f"   Validation accuracy: {val_accuracy:.3f}")
    print(f"   Gradient norm: {grad_norm:.3f}")
    print(f"   Epoch time: {epoch_time:.1f}s")
    print(f"   Learning rate: {trainer.optimizer.param_groups[0]['lr']:.2e}")
    
    # Show optimization features
    print(f"\nâœ… Optimizations Demonstrated:")
    print(f"   ğŸ¯ Advanced weight initialization applied")
    print(f"   ğŸ“ˆ Adaptive learning rate scheduling active")
    print(f"   ğŸ”„ Gradient accumulation working (steps: {trainer.accumulation_steps})")
    print(f"   ğŸ“Š EMA model averaging enabled")
    print(f"   ğŸ·ï¸ Label smoothing in loss function")
    print(f"   âš¡ Enhanced mixed precision training")
    print(f"   ğŸ“‰ Strong regularization (weight decay: {config.weight_decay})")
    
    # Memory and performance info
    if torch.cuda.is_available():
        memory_used = torch.cuda.max_memory_allocated() / 1024**2
        print(f"   ğŸ’¾ GPU memory used: {memory_used:.1f} MB")
    
    print(f"\nğŸŠ DEMONSTRATION COMPLETE!")
    print(f"âœ… All optimizations are working correctly")
    print(f"ğŸš€ Ready for full training with better convergence!")
    
    return True

def main():
    """Main demo function."""
    try:
        quick_optimization_demo()
        return 0
    except Exception as e:
        print(f"âŒ Demo failed: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
